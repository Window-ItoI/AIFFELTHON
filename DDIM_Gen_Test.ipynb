{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6be66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from keras.models import Sequential\n",
    "from PIL import Image    \n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc6c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tee: '/sys/bus/pci/devices/0000:00:04.0/numa_node': Read-only file system\r\n",
      "0\r\n"
     ]
    }
   ],
   "source": [
    "!echo 0 | sudo tee -a /sys/bus/pci/devices/0000\\:00\\:04.0/numa_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd075742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15206558746470766238,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 14474280960\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 15065085895464113902\n",
       " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c1b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mnist\"\n",
    "# 반복할 횟수(1이상으로 잡아야 돌아갑니다.)\n",
    "# dataset_repetitions = 3\n",
    "dataset_repetitions = 1\n",
    "# Epoch\n",
    "num_epochs = 50\n",
    "# num_epochs = 1\n",
    "\n",
    "# resize시 정할 이미지\n",
    "# image_size = 256\n",
    "image_size = 32\n",
    "\n",
    "# 데이터셋 컬러여부\n",
    "image_colored = 0\n",
    "# image_colored = 1\n",
    "channel = 3\n",
    "\n",
    "\n",
    "# KID = Kernel Inception Distance, see related section\n",
    "kid_image_size = 75\n",
    "# kid_image_size = 128\n",
    "# kid_diffusion_steps = 5\n",
    "kid_diffusion_steps = 10\n",
    "# plot_diffusion_steps = 20\n",
    "plot_diffusion_steps = 100\n",
    "# one_plot_diffusion_steps = 100\n",
    "one_plot_diffusion_steps = 100\n",
    "\n",
    "# sampling\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "\n",
    "# architecture\n",
    "embedding_dims = 32\n",
    "embedding_max_frequency = 1000.0\n",
    "widths = [64, 128, 194, 256]\n",
    "block_depth = 2\n",
    "\n",
    "# optimization\n",
    "batch_size = 64\n",
    "# batch_size = 16\n",
    "ema = 0.999\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# class\n",
    "num_class = 8\n",
    "class_embedding_dims = 32\n",
    "\n",
    "#dataset = tf.data.Dataset.from_generator(Gen,(tf.float32, tf.int8), ((256,256,6), (8)))\n",
    "#val_dataset = tf.data.Dataset.from_generator(Gen,(tf.float32, tf.int8), ((256,256,6),(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d98815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.kwangsiklee.com/2018/11/keras%EC%97%90%EC%84%9C-sequence%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%8C%80%EC%9A%A9%EB%9F%89-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EC%B2%98%EB%A6%AC%ED%95%98%EA%B8%B0/\n",
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class Gen(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, data_path, labels, batch_size= 32, n_channels=3, n_classes=8, shuffle=True, image_size = 256):\n",
    "        self.data_path, self.labels = data_path, labels# 데이터셋 경로, 데이터 라벨 리스트\n",
    "        self.batch_size = batch_size # 배치사이즈 설정\n",
    "        self.n_channels = n_channels # 채널 수\n",
    "        self.n_classes = n_classes # 클래스 갯수\n",
    "        self.shuffle = shuffle # 각 epoch마다 새로운 order를 만들어냄\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end() # 각 에포크의 맨 처음과 끝에 실행됨. \n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data_path) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes =  self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        print(indexes)\n",
    "        # list_IDs_temp = [self.labels[k] for k in indexes]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return (X, y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data_path))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)  \n",
    "\n",
    "    def crop_img (self, img, w, h):\n",
    "        start = ((w // 2 - 128) , (h // 2 - 128))\n",
    "        end = ((w // 2 + 128), (h // 2 + 128))\n",
    "        crop_img = img[start[1]:end[1], start[0]:end[0]]\n",
    "        return crop_img\n",
    "            \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.image_size, self.image_size, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int) # 메모리만 할당 받고 메모리 초기화는 하지 않는 함수 메모리 초기화를 하려면 zeros를 사용해야함.\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(indexes):\n",
    "            img = cv2.imread(self.data_path[ID])\n",
    "            h,w,c = img.shape\n",
    "            crop = self.crop_img(img, w, h)\n",
    "\n",
    "            X[i, ] = crop\n",
    "            y[i] = self.labels[ID]\n",
    "        return X, y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ca9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인용 코드\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "csv_path = 'aiffeltone/'\n",
    "csv_lists = glob(csv_path + '*.csv')\n",
    "\n",
    "def concat_csv(csv_lists):\n",
    "    \n",
    "    for i, csv in enumerate(csv_lists):\n",
    "        csv_df = pd.read_csv(csv)\n",
    "        if i == 0:\n",
    "            df = csv_df\n",
    "        else:\n",
    "            df = pd.concat([df, csv_df])\n",
    "        df = df.sample(frac = 1).reset_index(drop=True)\n",
    "    return df\n",
    "            \n",
    "def df_to_generator(dataframe):\n",
    "    data_path = list(dataframe['data_path'])\n",
    "    labels = list(dataframe['label'])\n",
    "\n",
    "    g = Gen(data_path, labels, batch_size=2)\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c897827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aiffeltone/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>aiffeltone/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>aiffeltone/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>aiffeltone/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>aiffeltone/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>aiffeltone/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>aiffeltone/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>aiffeltone/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>aiffeltone/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>aiffeltone/hanok.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                 data_path\n",
       "0      0  aiffeltone/building.jpeg\n",
       "1      5     aiffeltone/hanok.jpeg\n",
       "2      4  aiffeltone/building.jpeg\n",
       "3      7     aiffeltone/hanok.jpeg\n",
       "4      6  aiffeltone/building.jpeg\n",
       "5      3  aiffeltone/building.jpeg\n",
       "6      1  aiffeltone/building.jpeg\n",
       "7      2  aiffeltone/building.jpeg\n",
       "8      1     aiffeltone/hanok.jpeg\n",
       "9      3     aiffeltone/hanok.jpeg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>aiffeltone/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>aiffeltone/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>aiffeltone/hanok.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label              data_path\n",
       "10      4  aiffeltone/hanok.jpeg\n",
       "11      0  aiffeltone/hanok.jpeg\n",
       "12      2  aiffeltone/hanok.jpeg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe = concat_csv(csv_lists)\n",
    "print(len(dataframe))\n",
    "\n",
    "file_count = len(dataframe)\n",
    "\n",
    "train_df = dataframe[:int(file_count*0.8)]\n",
    "display(train_df)\n",
    "\n",
    "val_df = dataframe[int(file_count*0.8):]\n",
    "display(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e95d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n"
     ]
    }
   ],
   "source": [
    "train_gen = df_to_generator(train_df)\n",
    "# print(train_gen)\n",
    "\n",
    "# train_batch =  next(iter(train_gen))\n",
    "# print(train_batch)\n",
    "\n",
    "print('--------------')\n",
    "\n",
    "val_gen = df_to_generator(val_df)\n",
    "# print(val_gen)\n",
    "\n",
    "# val_batch =  next(iter(val_gen))\n",
    "# print(val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c906509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지의 너비와 높이 중 짧은 길이에 맞춰서 center crop하는 함수\n",
    "def preprocess_image(data):\n",
    "    c_vect = tf.one_hot(data[\"label\"], num_class)\n",
    "    # Center crop한 이미지를 Hyper parameter에서 정해준 image_size에 맞게 resize 해줍니다.\n",
    "    # resize시 이미지 훼손을 최대한 방지하기 위해 antialias를 True로 해줍니다.\n",
    "    image = tf.image.resize(image, size=[image_size, image_size], antialias=True)\n",
    "    \n",
    "    # 이미지 Array의 값을 float 0~1로 만들어주고 혹시 모를 특이값을 방지하기 위해 clip을 해줍니다.\n",
    "    return {'image': tf.clip_by_value(image / 255.0, 0.0, 1.0), 'label' : c_vect}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73927205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID 배치마다 계산되며 배치간의 값들로 평균을 냅니다.\n",
    "        self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(image_size, image_size, 3)),\n",
    "                layers.Rescaling(255.0),\n",
    "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b1d94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x):\n",
    "    embedding_min_frequency = 1.0\n",
    "    frequencies = tf.exp(\n",
    "        tf.linspace(\n",
    "            tf.math.log(embedding_min_frequency),\n",
    "            tf.math.log(embedding_max_frequency),\n",
    "            embedding_dims // 2,\n",
    "        )\n",
    "    )\n",
    "    angular_speeds = 2.0 * math.pi * frequencies\n",
    "    embeddings = tf.concat(\n",
    "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717971ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResidualBlock(width):\n",
    "    def apply(x):\n",
    "        input_width = x.shape[3]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
    "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n",
    "        )(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f87799f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width)(x)\n",
    "            skips.append(x)\n",
    "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def UpBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
    "        for _ in range(block_depth):\n",
    "            x = layers.Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f66fee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(image_size, widths, num_class, block_depth):\n",
    "    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n",
    "    noise_variances = keras.Input(shape=(1, 1, 1))\n",
    "    class_vector = keras.Input(shape = (num_class,))\n",
    "    \n",
    "    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n",
    "#     print('e :', e)\n",
    "    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n",
    "#     print('e :',e)\n",
    "    \n",
    "    i = layers.Dense(class_embedding_dims)(class_vector)\n",
    "#     print('i :',i)\n",
    "#     i = layers.Reshape((1,1,class_embedding_dims))(i)\n",
    "    i = i[:,None,None,:]\n",
    "#     print('i :',i)\n",
    "    i = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(i)\n",
    "#     print('i :',i)\n",
    "    \n",
    "    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n",
    "    x = layers.Concatenate()([x, i, e])\n",
    "#     print('x', x)\n",
    "\n",
    "    skips = []\n",
    "    for width in widths[:-1]:\n",
    "        x = DownBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    for _ in range(block_depth):\n",
    "        x = ResidualBlock(widths[-1])(x)\n",
    "\n",
    "    for width in reversed(widths[:-1]):\n",
    "        x = UpBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "#     x = layers.Conv2D(1, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "    \n",
    "\n",
    "    return keras.Model([noisy_images, noise_variances, class_vector], x, name=\"residual_unet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a81dc0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(keras.Model):\n",
    "    def __init__(self, image_size, widths, num_class, block_depth):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalizer = layers.Normalization()\n",
    "        self.network = get_network(image_size, widths, num_class, block_depth)\n",
    "        self.ema_network = keras.models.clone_model(self.network)\n",
    "        \n",
    "    def call(self, data, training=False):\n",
    "        pass\n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n",
    "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
    "        self.kid = KID(name=\"kid\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]\n",
    "\n",
    "    def denormalize(self, images):\n",
    "        # convert the pixel values back to 0-1 range\n",
    "        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n",
    "        return tf.clip_by_value(images, 0.0, 1.0)\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        # diffusion times -> angles\n",
    "        start_angle = tf.acos(max_signal_rate)\n",
    "        end_angle = tf.acos(min_signal_rate)\n",
    "\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "        # angles -> signal and noise rates\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n",
    "\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates, class_vector, training):\n",
    "        # print(\"denoise start\")\n",
    "        # the exponential moving average weights are used at evaluation\n",
    "        if training:\n",
    "            network = self.network\n",
    "        else:\n",
    "            network = self.ema_network\n",
    "\n",
    "        # predict noise component and calculate the image component using it\n",
    "        # pred_noises = network([noisy_images, noise_rates**2], training=training)\n",
    "        pred_noises = network([noisy_images, noise_rates**2, class_vector], training=training)\n",
    "        # print(\"denoise get noises\")\n",
    "    \n",
    "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "        # print(\"denoise get images\") \n",
    "        \n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps, class_vector):\n",
    "        # reverse diffusion = sampling\n",
    "        num_images = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # important line:\n",
    "        # at the first sampling step, the \"noisy image\" is pure noise\n",
    "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
    "        next_noisy_images = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            noisy_images = next_noisy_images\n",
    "\n",
    "            # separate the current noisy image to its components\n",
    "            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, class_vector, training=False\n",
    "            )\n",
    "            # network used in eval mode\n",
    "\n",
    "            # remix the predicted components using the next signal and noise rates\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            next_noisy_images = (\n",
    "                next_signal_rates * pred_images + next_noise_rates * pred_noises\n",
    "            )\n",
    "            # this new noisy image will be used in the next step\n",
    "\n",
    "        return pred_images\n",
    "\n",
    "    def generate(self, num_images, diffusion_steps, class_vector):\n",
    "        # noise -> images -> denormalized images\n",
    "        initial_noise = tf.random.normal(shape=(num_images, image_size, image_size, 3))\n",
    "        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps, class_vector)\n",
    "        generated_images = self.denormalize(generated_images)\n",
    "        return generated_images\n",
    "\n",
    "    # def train_step(self, images):\n",
    "    def train_step(self, data):\n",
    "        # normalize images to have standard deviation of 1, like the noises\n",
    "        # print(\"train_step start\")\n",
    "        \n",
    "#         images = data['image']\n",
    "#         label = data['label']\n",
    "        images, label = data\n",
    "        # print(label)\n",
    "        images = self.normalizer(images, training=True)\n",
    "        noises = tf.random.normal(shape=(batch_size, image_size, image_size, 3))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # mix the images with noises accordingly\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        # print(\"train_step here 1\")\n",
    "        # print(\"noise_rates :\", noise_rates)\n",
    "        # print(\"signal_rates :\", signal_rates)\n",
    "        # print(\"noisy_images :\", noisy_images)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # train the network to separate noisy images to their components\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, label, training=True\n",
    "            )\n",
    "            # print('start get loss')\n",
    "            noise_loss = self.loss(noises, pred_noises)  # used for training\n",
    "            # print('get noise loss')\n",
    "            image_loss = self.loss(images, pred_images)  # only used as metric\n",
    "            # print('get image loss')\n",
    "            \n",
    "            \n",
    "        # print(\"train_step here 2\")\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "        self.image_loss_tracker.update_state(image_loss)\n",
    "\n",
    "        # track the exponential moving averages of weights\n",
    "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "    \n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics[:-1]}\n",
    "\n",
    "    # def test_step(self, images):\n",
    "    def test_step(self, data):\n",
    "        images = data['image']\n",
    "        label = data['label']\n",
    "        # normalize images to have standard deviation of 1, like the noises\n",
    "        images = self.normalizer(images, training=False)\n",
    "        noises = tf.random.normal(shape=(batch_size, image_size, image_size, 3))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # mix the images with noises accordingly\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        # use the network to separate noisy images to their components\n",
    "        pred_noises, pred_images = self.denoise(\n",
    "            noisy_images, noise_rates, signal_rates, label, training=False\n",
    "        )\n",
    "\n",
    "        noise_loss = self.loss(noises, pred_noises)\n",
    "        image_loss = self.loss(images, pred_images)\n",
    "\n",
    "        self.image_loss_tracker.update_state(image_loss)\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "\n",
    "        # measure KID between real and generated images\n",
    "        # this is computationally demanding, kid_diffusion_steps has to be small\n",
    "        images = self.denormalize(images)\n",
    "        generated_images = self.generate(\n",
    "            num_images=batch_size, diffusion_steps=kid_diffusion_steps, class_vector = label\n",
    "        )\n",
    "        self.kid.update_state(images, generated_images)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    # plot random generated images for visual evaluation of generation quality\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=2, num_cols=5):\n",
    "    # def plot_images(self, epoch=None, logs=None, num_rows=1, num_cols=5):\n",
    "        \n",
    "        indices = [0,1,2,3,4,5,6,7,8,9]\n",
    "        # indices = [0,1,2,3,4]\n",
    "        sample_vector = tf.one_hot(indices, num_class)\n",
    "        # sample_vector shape (10,10)\n",
    "        generated_images = self.generate(\n",
    "            num_images=num_rows * num_cols,\n",
    "            diffusion_steps=plot_diffusion_steps,\n",
    "            class_vector =  sample_vector \n",
    "        )\n",
    "        # generated_images shape : (10,32,32,3)\n",
    "\n",
    "        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                index = row * num_cols + col\n",
    "                plt.subplot(num_rows, num_cols, index + 1)\n",
    "                plt.imshow(generated_images[index])\n",
    "                plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_one_images(self, input_class = 0, epoch=None, logs=None):\n",
    "        \n",
    "        index = [input_class]\n",
    "        sample_vector = tf.one_hot(index, num_class)\n",
    "        # sample_vector shape : (1,10)\n",
    "            \n",
    "        generated_images = self.generate(\n",
    "            num_images=1,\n",
    "            diffusion_steps = one_plot_diffusion_steps,\n",
    "            class_vector =  sample_vector \n",
    "        )\n",
    "        # generated_images shape : (1,32,32,3)\n",
    "        \n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(generated_images[0])\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d5c06f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.9/site-packages (0.14.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.9/site-packages (from tensorflow_addons) (2.13.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42aabf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "# create and compile the model\n",
    "model = DiffusionModel(image_size, widths, num_class, block_depth)\n",
    "# below tensorflow 2.9:\n",
    "# pip install tensorflow_addons\n",
    "# import tensorflow_addons as tfa\n",
    "# optimizer=tfa.optimizers.AdamW\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(\n",
    "#     optimizer=keras.optimizers.experimental.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "# pixelwise mean absolute error is used as loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8bbf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model based on the validation KID metric\n",
    "checkpoint_path = \"checkpoint/diffusion_model\"\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_kid\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b162bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4]\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /tmp/ipykernel_310/3257888640.py:108 train_step\n        images = self.normalizer(images, training=True)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1030 __call__\n        self._maybe_build(inputs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:2659 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/normalization.py:145 build\n        raise ValueError(\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None, None, None, None], with unknown axis at index: 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_310/3178346283.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#train_image = train_dataset.map(lambda x : x['image'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.normalizer.adapt(train_image)# run training and plot generated images periodically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /tmp/ipykernel_310/3257888640.py:108 train_step\n        images = self.normalizer(images, training=True)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1030 __call__\n        self._maybe_build(inputs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:2659 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/normalization.py:145 build\n        raise ValueError(\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None, None, None, None], with unknown axis at index: 3\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and variance of training dataset for normalization\n",
    "# 이미지와 label로 데이터셋의 형태가 변하였기 때문에 image만 가져와서 adapt 시켜줍니다.\n",
    "#train_image = train_dataset.map(lambda x : x['image'])\n",
    "#model.normalizer.adapt(train_image)# run training and plot generated images periodically\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=[\n",
    "        keras.callbacks.LambdaCallback(on_epoch_end = model.plot_images),\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00616bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model and generate images\n",
    "model.load_weights(checkpoint_path)\n",
    "model.plot_images()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

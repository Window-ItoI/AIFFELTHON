{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a622170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from keras.models import Sequential\n",
    "from PIL import Image    \n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ea8dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/paul025220/anaconda3/envs/itoi/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "0\r\n"
     ]
    }
   ],
   "source": [
    "!echo 0 | sudo tee -a /sys/bus/pci/devices/0000\\:00\\:04.0/numa_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c859b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 09:33:06.915703: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-07 09:33:07.758658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 14640 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9177844548952321424\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 15352135680\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 9174408838323690371\n",
       " physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28311f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"mnist\"\n",
    "# 반복할 횟수(1이상으로 잡아야 돌아갑니다.)\n",
    "# dataset_repetitions = 3\n",
    "dataset_repetitions = 1\n",
    "# Epoch\n",
    "num_epochs = 50\n",
    "# num_epochs = 1\n",
    "\n",
    "# resize시 정할 이미지\n",
    "# image_size = 256\n",
    "image_size = 256\n",
    "# 데이터셋 컬러여부\n",
    "# image_colored = 0\n",
    "image_colored = 1\n",
    "channel = 3\n",
    "\n",
    "\n",
    "# KID = Kernel Inception Distance, see related section\n",
    "kid_image_size = 75\n",
    "# kid_image_size = 128\n",
    "kid_diffusion_steps = 5\n",
    "# kid_diffusion_steps = 10\n",
    "plot_diffusion_steps = 20\n",
    "# plot_diffusion_steps = 100\n",
    "one_plot_diffusion_steps = 20\n",
    "# one_plot_diffusion_steps = 100\n",
    "\n",
    "# sampling\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "\n",
    "# architecture\n",
    "embedding_dims = 32\n",
    "embedding_max_frequency = 1000.0\n",
    "widths = [32, 64, 96, 128]\n",
    "block_depth = 2\n",
    "\n",
    "# optimization\n",
    "batch_size = 1\n",
    "# batch_size = 16\n",
    "ema = 0.999\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# class\n",
    "num_class = 2\n",
    "class_embedding_dims = 32\n",
    "\n",
    "#dataset = tf.data.Dataset.from_generator(Gen,(tf.float32, tf.int8), ((256,256,6), (8)))\n",
    "#val_dataset = tf.data.Dataset.from_generator(Gen,(tf.float32, tf.int8), ((256,256,6),(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db1bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.kwangsiklee.com/2018/11/keras%EC%97%90%EC%84%9C-sequence%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%8C%80%EC%9A%A9%EB%9F%89-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EC%B2%98%EB%A6%AC%ED%95%98%EA%B8%B0/\n",
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class Gen(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, data_path, labels, batch_size= batch_size, n_channels=channel, n_classes=num_class, shuffle=True, image_size = image_size):\n",
    "        self.data_path, self.labels = data_path, labels# 데이터셋 경로, 데이터 라벨 리스트\n",
    "        self.batch_size = batch_size # 배치사이즈 설정\n",
    "        self.n_channels = n_channels # 채널 수\n",
    "        self.n_classes = n_classes # 클래스 갯수\n",
    "        self.shuffle = shuffle # 각 epoch마다 새로운 order를 만들어냄\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end() # 각 에포크의 맨 처음과 끝에 실행됨. \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data_path) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print('index : ', index)\n",
    "        indexes =  self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        print(self.indexes)\n",
    "        print('indexes : ', indexes)\n",
    "        # list_IDs_temp = [self.labels[k] for k in indexes]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        y = tf.one_hot(indices = y, depth = self.n_classes, dtype = tf.int64)\n",
    "        y = np.array(y)\n",
    "        print(\"X type\", type(X))\n",
    "        print(\"y type\", type(y))\n",
    "        \n",
    "        return (X, y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data_path))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)  \n",
    "\n",
    "    def crop_img (self, img, w, h):\n",
    "        start = ((w // 2 - 128) , (h // 2 - 128))\n",
    "        end = ((w // 2 + 128), (h // 2 + 128))\n",
    "        crop_img = img[start[1]:end[1], start[0]:end[0]]\n",
    "        return np.clip(crop_img/255.0, 0.0, 1.0)\n",
    "            \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.image_size, self.image_size, self.n_channels))\n",
    "        y = np.empty((self.batch_size) , dtype = int) # 메모리만 할당 받고 메모리 초기화는 하지 않는 함수 메모리 초기화를 하려면 zeros를 사용해야함.\n",
    "        print('self.batch_size :', self.batch_size)\n",
    "        print('X.shape :', X.shape)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(indexes):\n",
    "            img = cv2.imread(self.data_path[ID])\n",
    "            h,w,c = img.shape\n",
    "            crop = self.crop_img(img, w, h)\n",
    "\n",
    "            X[i, ] = crop\n",
    "            y[i] = self.labels[ID]\n",
    "        return X, y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c739070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인용 코드\n",
    "def concat_csv(csv_lists):\n",
    "    \n",
    "    for i, csv in enumerate(csv_lists):\n",
    "        csv_df = pd.read_csv(csv)\n",
    "        if i == 0:\n",
    "            df = csv_df\n",
    "        else:\n",
    "            df = pd.concat([df, csv_df])\n",
    "        df = df.sample(frac = 1).reset_index(drop=True)\n",
    "    return df\n",
    "            \n",
    "def df_to_generator(dataframe):\n",
    "    data_path = list(dataframe['data_path'])\n",
    "    labels = list(dataframe['label'])\n",
    "\n",
    "    g = Gen(data_path, labels, batch_size)\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ecbf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_count : 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sample/hanok2.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>sample/hanok2.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>sample/hanok2.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>sample/hanok2.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>sample/hanok2.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label             data_path\n",
       "0       1     sample/hanok.jpeg\n",
       "1       2    sample/hanok2.jpeg\n",
       "2       0  sample/building.jpeg\n",
       "3       2    sample/hanok2.jpeg\n",
       "4       0  sample/building.jpeg\n",
       "5       0  sample/building.jpeg\n",
       "6       1     sample/hanok.jpeg\n",
       "7       1     sample/hanok.jpeg\n",
       "8       1     sample/hanok.jpeg\n",
       "9       2    sample/hanok2.jpeg\n",
       "10      1     sample/hanok.jpeg\n",
       "11      2    sample/hanok2.jpeg\n",
       "12      1     sample/hanok.jpeg\n",
       "13      0  sample/building.jpeg\n",
       "14      1     sample/hanok.jpeg\n",
       "15      0  sample/building.jpeg\n",
       "16      1     sample/hanok.jpeg\n",
       "17      1     sample/hanok.jpeg\n",
       "18      1     sample/hanok.jpeg\n",
       "19      0  sample/building.jpeg\n",
       "20      0  sample/building.jpeg\n",
       "21      0  sample/building.jpeg\n",
       "22      1     sample/hanok.jpeg\n",
       "23      2    sample/hanok2.jpeg\n",
       "24      0  sample/building.jpeg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>sample/hanok2.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>sample/hanok.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>sample/building.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>sample/hanok2.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label             data_path\n",
       "25      2    sample/hanok2.jpeg\n",
       "26      1     sample/hanok.jpeg\n",
       "27      1     sample/hanok.jpeg\n",
       "28      0  sample/building.jpeg\n",
       "29      0  sample/building.jpeg\n",
       "30      0  sample/building.jpeg\n",
       "31      2    sample/hanok2.jpeg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path = 'sample/'\n",
    "csv_lists = glob(csv_path + '*.csv')\n",
    "\n",
    "dataframe = concat_csv(csv_lists)\n",
    "file_count = len(dataframe)\n",
    "print('file_count :', file_count)\n",
    "train_df = dataframe[:int(file_count*0.8)]\n",
    "display(train_df)\n",
    "\n",
    "val_df = dataframe[int(file_count*0.8):]\n",
    "display(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f22d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = df_to_generator(train_df)\n",
    "# print(train_gen)\n",
    "# train_batch =  next(iter(train_gen))\n",
    "# print(train_batch)\n",
    "\n",
    "# print('--------------')\n",
    "\n",
    "val_gen = df_to_generator(val_df)\n",
    "# print(val_gen)\n",
    "\n",
    "# val_batch  = next(iter(val_gen))\n",
    "# print(val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f074a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Gen object at 0x7fbae0307040>\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(train_gen)\n",
    "print(len(train_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed85a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :  0\n",
      "indexes :  [7]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "-------\n",
      "index :  0\n",
      "indexes :  [7]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 09:33:19.595356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14640 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_gen))[0].shape)\n",
    "print(\"-------\")\n",
    "print(next(iter(train_gen))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f128606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :  0\n",
      "indexes :  [7]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  1\n",
      "indexes :  [9]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  2\n",
      "indexes :  [11]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  3\n",
      "indexes :  [5]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  4\n",
      "indexes :  [14]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  5\n",
      "indexes :  [12]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  6\n",
      "indexes :  [8]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  7\n",
      "indexes :  [1]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  8\n",
      "indexes :  [6]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  9\n",
      "indexes :  [18]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  10\n",
      "indexes :  [23]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  11\n",
      "indexes :  [2]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  12\n",
      "indexes :  [4]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  13\n",
      "indexes :  [20]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  14\n",
      "indexes :  [16]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  15\n",
      "indexes :  [19]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  16\n",
      "indexes :  [15]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  17\n",
      "indexes :  [10]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  18\n",
      "indexes :  [22]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  19\n",
      "indexes :  [21]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  20\n",
      "indexes :  [24]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  21\n",
      "indexes :  [3]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  22\n",
      "indexes :  [13]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  23\n",
      "indexes :  [0]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n",
      "index :  24\n",
      "indexes :  [17]\n",
      "self.batch_size : 1\n",
      "X.shape : (1, 256, 256, 3)\n",
      "X type <class 'numpy.ndarray'>\n",
      "y type <class 'numpy.ndarray'>\n",
      "(1, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "for t in iter(train_gen):\n",
    "    print(t[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd01ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지의 너비와 높이 중 짧은 길이에 맞춰서 center crop하는 함수\n",
    "def preprocess_image(data):\n",
    "    c_vect = tf.one_hot(data[\"label\"], num_class)\n",
    "    # Center crop한 이미지를 Hyper parameter에서 정해준 image_size에 맞게 resize 해줍니다.\n",
    "    # resize시 이미지 훼손을 최대한 방지하기 위해 antialias를 True로 해줍니다.\n",
    "    image = tf.image.resize(image, size=[image_size, image_size], antialias=True)\n",
    "    \n",
    "    # 이미지 Array의 값을 float 0~1로 만들어주고 혹시 모를 특이값을 방지하기 위해 clip을 해줍니다.\n",
    "    return {'image': tf.clip_by_value(image / 255.0, 0.0, 1.0), 'label' : c_vect}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89dea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID 배치마다 계산되며 배치간의 값들로 평균을 냅니다.\n",
    "        self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(image_size, image_size, 3)),\n",
    "                layers.Rescaling(255.0),\n",
    "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x):\n",
    "    embedding_min_frequency = 1.0\n",
    "    frequencies = tf.exp(\n",
    "        tf.linspace(\n",
    "            tf.math.log(embedding_min_frequency),\n",
    "            tf.math.log(embedding_max_frequency),\n",
    "            embedding_dims // 2,\n",
    "        )\n",
    "    )\n",
    "    angular_speeds = 2.0 * math.pi * frequencies\n",
    "    embeddings = tf.concat(\n",
    "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dc94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResidualBlock(width):\n",
    "    def apply(x):\n",
    "        input_width = x.shape[3]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
    "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n",
    "        )(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width)(x)\n",
    "            skips.append(x)\n",
    "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def UpBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
    "        for _ in range(block_depth):\n",
    "            x = layers.Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(image_size, widths, num_class, block_depth):\n",
    "    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n",
    "    noise_variances = keras.Input(shape=(1, 1, 1))\n",
    "    class_vector = keras.Input(shape = (num_class,))\n",
    "    \n",
    "    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n",
    "#     print('e :', e)\n",
    "    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n",
    "#     print('e :',e)\n",
    "    \n",
    "    i = layers.Dense(class_embedding_dims)(class_vector)\n",
    "#     print('i :',i)\n",
    "#     i = layers.Reshape((1,1,class_embedding_dims))(i)\n",
    "    i = i[:,None,None,:]\n",
    "#     print('i :',i)\n",
    "    i = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(i)\n",
    "#     print('i :',i)\n",
    "    \n",
    "    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n",
    "    x = layers.Concatenate()([x, i, e])\n",
    "#     print('x', x)\n",
    "\n",
    "    skips = []\n",
    "    for width in widths[:-1]:\n",
    "        x = DownBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    for _ in range(block_depth):\n",
    "        x = ResidualBlock(widths[-1])(x)\n",
    "\n",
    "    for width in reversed(widths[:-1]):\n",
    "        x = UpBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "#     x = layers.Conv2D(1, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "    \n",
    "\n",
    "    return keras.Model([noisy_images, noise_variances, class_vector], x, name=\"residual_unet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(keras.Model):\n",
    "    def __init__(self, image_size, widths, num_class, block_depth):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalizer = layers.Normalization()\n",
    "        self.network = get_network(image_size, widths, num_class, block_depth)\n",
    "        self.ema_network = keras.models.clone_model(self.network)\n",
    "        \n",
    "#     def call(self, data, training=False):\n",
    "        \n",
    "#         pass\n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n",
    "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
    "        self.kid = KID(name=\"kid\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]\n",
    "\n",
    "    def denormalize(self, images):\n",
    "        # convert the pixel values back to 0-1 range\n",
    "        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n",
    "        return tf.clip_by_value(images, 0.0, 1.0)\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        # diffusion times -> angles\n",
    "        start_angle = tf.acos(max_signal_rate)\n",
    "        end_angle = tf.acos(min_signal_rate)\n",
    "\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "        # angles -> signal and noise rates\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n",
    "\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates, class_vector, training):\n",
    "        # print(\"denoise start\")\n",
    "        # the exponential moving average weights are used at evaluation\n",
    "        if training:\n",
    "            network = self.network\n",
    "        else:\n",
    "            network = self.ema_network\n",
    "\n",
    "        # predict noise component and calculate the image component using it\n",
    "        # pred_noises = network([noisy_images, noise_rates**2], training=training)\n",
    "        pred_noises = network([noisy_images, noise_rates**2, class_vector], training=training)\n",
    "        # print(\"denoise get noises\")\n",
    "    \n",
    "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "        # print(\"denoise get images\") \n",
    "        \n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps, class_vector):\n",
    "        # reverse diffusion = sampling\n",
    "        num_images = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # important line:\n",
    "        # at the first sampling step, the \"noisy image\" is pure noise\n",
    "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
    "        next_noisy_images = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            noisy_images = next_noisy_images\n",
    "\n",
    "            # separate the current noisy image to its components\n",
    "            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, class_vector, training=False\n",
    "            )\n",
    "            # network used in eval mode\n",
    "\n",
    "            # remix the predicted components using the next signal and noise rates\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            next_noisy_images = (\n",
    "                next_signal_rates * pred_images + next_noise_rates * pred_noises\n",
    "            )\n",
    "            # this new noisy image will be used in the next step\n",
    "\n",
    "        return pred_images\n",
    "\n",
    "    def generate(self, num_images, diffusion_steps, class_vector):\n",
    "        # noise -> images -> denormalized images\n",
    "        initial_noise = tf.random.normal(shape=(num_images, image_size, image_size, 3))\n",
    "        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps, class_vector)\n",
    "        generated_images = self.denormalize(generated_images)\n",
    "        return generated_images\n",
    "\n",
    "    # def train_step(self, images):\n",
    "    def train_step(self, data):\n",
    "        # normalize images to have standard deviation of 1, like the noises\n",
    "        # print(\"train_step start\")\n",
    "        \n",
    "        #images = data['image']\n",
    "        #label = data['label']\n",
    "        print('data :', data)\n",
    "        image, label = data\n",
    "        print('images :', image)\n",
    "        print('label :', label)\n",
    "        \n",
    "        print('image.shape :', images.shape)\n",
    "        \n",
    "        images = self.normalizer(images, training = True)\n",
    "        noises = tf.random.normal(shape=(batch_size, image_size, image_size, 3))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # mix the images with noises accordingly\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        # print(\"train_step here 1\")\n",
    "        # print(\"noise_rates :\", noise_rates)\n",
    "        # print(\"signal_rates :\", signal_rates)\n",
    "        # print(\"noisy_images :\", noisy_images)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # train the network to separate noisy images to their components\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, label, training=True\n",
    "            )\n",
    "            # print('start get loss')\n",
    "            noise_loss = self.loss(noises, pred_noises)  # used for training\n",
    "            # print('get noise loss')\n",
    "            image_loss = self.loss(images, pred_images)  # only used as metric\n",
    "            # print('get image loss')\n",
    "            \n",
    "            \n",
    "        # print(\"train_step here 2\")\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "        self.image_loss_tracker.update_state(image_loss)\n",
    "\n",
    "        # track the exponential moving averages of weights\n",
    "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "    \n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics[:-1]}\n",
    "\n",
    "    # def test_step(self, images):\n",
    "    def test_step(self, data):\n",
    "#         images = data['image']\n",
    "#         label = data['label']\n",
    "        images, label = data\n",
    "        # normalize images to have standard deviation of 1, like the noises\n",
    "        images = self.normalizer(images, training=False)\n",
    "        noises = tf.random.normal(shape=(batch_size, image_size, image_size, 3))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # mix the images with noises accordingly\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        # use the network to separate noisy images to their components\n",
    "        pred_noises, pred_images = self.denoise(\n",
    "            noisy_images, noise_rates, signal_rates, label, training=False\n",
    "        )\n",
    "\n",
    "        noise_loss = self.loss(noises, pred_noises)\n",
    "        image_loss = self.loss(images, pred_images)\n",
    "\n",
    "        self.image_loss_tracker.update_state(image_loss)\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "\n",
    "        # measure KID between real and generated images\n",
    "        # this is computationally demanding, kid_diffusion_steps has to be small\n",
    "        images = self.denormalize(images)\n",
    "        generated_images = self.generate(\n",
    "            num_images=batch_size, diffusion_steps=kid_diffusion_steps, class_vector = label\n",
    "        )\n",
    "        self.kid.update_state(images, generated_images)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    # plot random generated images for visual evaluation of generation quality\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=2, num_cols=1):\n",
    "    # def plot_images(self, epoch=None, logs=None, num_rows=1, num_cols=5):\n",
    "        \n",
    "        # indices = [0,1,2,3,4,5,6,7,8,9]\n",
    "        indices = [0,1]\n",
    "        sample_vector = tf.one_hot(indices, num_class)\n",
    "        # sample_vector shape (10,10)\n",
    "        generated_images = self.generate(\n",
    "            num_images=num_rows * num_cols,\n",
    "            diffusion_steps=plot_diffusion_steps,\n",
    "            class_vector =  sample_vector \n",
    "        )\n",
    "        # generated_images shape : (10,32,32,3)\n",
    "\n",
    "        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                index = row * num_cols + col\n",
    "                plt.subplot(num_rows, num_cols, index + 1)\n",
    "                plt.imshow(generated_images[index])\n",
    "                plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_one_images(self, input_class = 0, epoch=None, logs=None):\n",
    "        \n",
    "        index = [input_class]\n",
    "        sample_vector = tf.one_hot(index, num_class)\n",
    "        # sample_vector shape : (1,10)\n",
    "            \n",
    "        generated_images = self.generate(\n",
    "            num_images=1,\n",
    "            diffusion_steps = one_plot_diffusion_steps,\n",
    "            class_vector =  sample_vector \n",
    "        )\n",
    "        # generated_images shape : (1,32,32,3)\n",
    "        \n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(generated_images[0])\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbd350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "# create and compile the model\n",
    "model = DiffusionModel(image_size, widths, num_class, block_depth)\n",
    "# below tensorflow 2.9:\n",
    "# pip install tensorflow_addons\n",
    "# import tensorflow_addons as tfa\n",
    "# optimizer=tfa.optimizers.AdamW\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(\n",
    "#     optimizer=keras.optimizers.experimental.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "# pixelwise mean absolute error is used as loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model based on the validation KID metric\n",
    "checkpoint_path = \"checkpoint/diffusion_model\"\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_kid\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and variance of training dataset for normalization\n",
    "# 이미지와 label로 데이터셋의 형태가 변하였기 때문에 image만 가져와서 adapt 시켜줍니다.\n",
    "# train_image = train_gen.map(lambda x : x, y)\n",
    "# model.normalizer.adapt(train_gen)# run training and plot generated images periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24768f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_gen,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=[\n",
    "        keras.callbacks.LambdaCallback(on_epoch_end = model.plot_images),\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9252c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model and generate images\n",
    "model.load_weights(checkpoint_path)\n",
    "model.plot_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff128c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
